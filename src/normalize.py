import pynini
from pynini.lib import pynutil, utf8
# from normalize_en  import get_normilizer as normalize_en
from utils import apply_fst, far_path


# retrieve stored fst
archive = pynini.Far(far_path) 

english_fsts = archive['en_fst']
french_fsts = archive['fre_fst']


#  Some variables and functions used in generating the normalized text

digits = pynini.closure(pynini.union(*[pynini.accep(chr(i)) for i in range(ord('0'), ord('9')+1)]), 1)
letters = pynini.closure(pynini.union(*[pynini.accep(chr(i)) for i in range(ord('a'), ord('z')+1)] +
                                       [pynini.accep(chr(i)) for i in range(ord('A'), ord('Z')+1)]), 1)
apostrophe = pynini.union("'", "’")

word = letters + pynini.closure(apostrophe + letters)  # e.g. n'iminota, can't, etc.
number = digits
punctuation = pynini.union(*[pynini.accep('\\' +c) for c in ".,:;!?-(){}[]\""])

token = number | word | punctuation
def tokenize_pynini(text: str) -> list[str]:
    lattice = pynini.accep(text)
    tokens = []
    start = 0
    while start < len(text):
        # Try to find the longest token starting at `start`
        longest_token = None
        longest_len = 0
        for length in range(1, len(text) - start + 1):
            segment = text[start:start+length]
            if pynini.compose(segment, token).num_states() > 0:
                # Accept if segment matches token
                longest_token = segment
                longest_len = length
        if longest_token is None:
            # No token found, consume one char as unknown token
            longest_token = text[start]
            longest_len = 1
        tokens.append(longest_token)
        start += longest_len
    return tokens

# This part takes the individual tokens generated by the tokenizer and iteratively apply the FST to convert number to text.:
# it returns values that looks like this: { word { value: "we" } }  { cardinal { integer: "six" } }}. Where different classes have different tags, and only the chooses classes are affected by the normalizer.
# in our case only the values with tags "cardinal"
def classify_token( token: str) -> str:
    print(type(token))
    if token.isdigit():
        verbalized = apply_fst(token,fsts)
        # verbalized = pynini.shortestpath(
            # pynini.accep(token, token_type="utf8") @ rewrite_fst
        # ).string(token_type="utf8")
        return f'{{ cardinal {{ integer: "{verbalized}" }} }}'
    else:
        return f'{{ word {{ value: "{token}" }} }}'

def classify_sentence(text: str) -> str:
    tokens = tokenize_pynini(text)
    return " ".join(classify_token(token) for token in tokens)



QUOTE = pynini.accep('"')
NOT_QUOTE = pynini.difference(utf8.VALID_UTF8_CHAR, QUOTE)
VALUE = pynini.closure(NOT_QUOTE, 1)

# Match: { word { value: "<value>" } }
word_tag = pynutil.delete('{ word { value: "') + VALUE + pynutil.delete('" } }')

# Match: { cardinal { integer: "<value>" } }
cardinal_tag = pynutil.delete('{ cardinal { integer: "') + VALUE + pynutil.delete('" } }')

# Combine and rewrite globally
space_token = pynutil.delete('{ word { value: "')+ VALUE + pynutil.delete('" } }')
SPACE_BETWEEN_TAGS = pynutil.delete(" ")
tag_remover = pynini.union(word_tag, cardinal_tag, space_token,SPACE_BETWEEN_TAGS)
cleaner_fst = pynini.cdrewrite(tag_remover, "", "", pynini.closure(utf8.VALID_UTF8_CHAR)).optimize()


def strip_tags_with_pynutil(tagged_text: str) -> str:
    input_fst = pynini.accep(tagged_text, token_type="utf8")
    output_fst = input_fst @ cleaner_fst
    if output_fst.start() == pynini.NO_STATE_ID:
        raise ValueError("Rewrite failed — check input format.")
    return pynini.shortestpath(output_fst).string(token_type="utf8")


def normalize_text( text = "we started with the number 100 and ended up 823."):

    normalized_with_tag = classify_sentence(text)
    normalized = strip_tags_with_pynutil(normalized_with_tag)
    print("text before normalization: ",text)
    # print("text normalized - but still taged: ", normalized_with_tag)
    print("text normalized: ", normalized)


normalize_text()

