'''
This file lets you normalize the numbers in text in either English or French
'''
import time
import argparse
import pynini
from pynini.lib import pynutil, utf8
from utils import apply_fst, far_path


# retrieve stored fst
archive = pynini.Far(far_path) 

#  Some variables and functions used in generating the normalized text

digits = pynini.closure(pynini.union(*[pynini.accep(chr(i)) for i in range(ord('0'), ord('9')+1)]), 1)
letters = pynini.closure(pynini.union(*[pynini.accep(chr(i)) for i in range(ord('a'), ord('z')+1)] +
                                       [pynini.accep(chr(i)) for i in range(ord('A'), ord('Z')+1)]), 1)
apostrophe = pynini.union("'", "’")

word = letters + pynini.closure(apostrophe + letters)  # e.g. n'iminota, can't, etc.
number = digits
punctuation = pynini.union(*[pynini.accep('\\' +c) for c in ".,:;!?-(){}[]\""])

token = number | word | punctuation
def tokenize_pynini(text: str) -> list[str]:
    tokens = []
    start = 0
    while start < len(text):
        # Try to find the longest token starting at `start`
        longest_token = None
        longest_len = 0
        for length in range(1, len(text) - start + 1):
            segment = text[start:start+length]
            if pynini.compose(segment, token).num_states() > 0:
                # Accept if segment matches token
                longest_token = segment
                longest_len = length
        if longest_token is None:
            # No token found, consume one char as unknown token
            longest_token = text[start]
            longest_len = 1
        tokens.append(longest_token)
        start += longest_len
    return tokens

# This part takes the individual tokens generated by the tokenizer and iteratively apply the FST to convert number to text.:
# it returns values that looks like this: { word { value: "we" } }  { cardinal { integer: "six" } }}. Where different classes have different tags, and only the chooses classes are affected by the normalizer.
# in our case only the values with tags "cardinal"
def classify_token( token: str, lang:str) -> str:
    if token.isdigit():
        
        verbalized = apply_fst(token, archive[lang])
        
        return f'{{ cardinal {{ integer: "{verbalized}" }} }}'
    else:
        return f'{{ word {{ value: "{token}" }} }}'

def classify_sentence(text: str, lang: str) -> str:
    lang_name = lang +'_fst'
    print(lang_name)
    tokens = tokenize_pynini(text)
    return " ".join(classify_token(token, lang_name) for token in tokens)



QUOTE = pynini.accep('"')
NOT_QUOTE = pynini.difference(utf8.VALID_UTF8_CHAR, QUOTE)
VALUE = pynini.closure(NOT_QUOTE, 1)

# Match: { word { value: "<value>" } }
word_tag = pynutil.delete('{ word { value: "') + VALUE + pynutil.delete('" } }')

# Match: { cardinal { integer: "<value>" } }
cardinal_tag = pynutil.delete('{ cardinal { integer: "') + VALUE + pynutil.delete('" } }')

# Combine and rewrite globally
space_token = pynutil.delete('{ word { value: "')+ VALUE + pynutil.delete('" } }')
SPACE_BETWEEN_TAGS = pynutil.delete(" ")
tag_remover = pynini.union(word_tag, cardinal_tag, space_token,SPACE_BETWEEN_TAGS)
cleaner_fst = pynini.cdrewrite(tag_remover, "", "", pynini.closure(utf8.VALID_UTF8_CHAR)).optimize()


def strip_tags_with_pynutil(tagged_text: str) -> str:
    input_fst = pynini.accep(tagged_text, token_type="utf8")
    output_fst = input_fst @ cleaner_fst
    if output_fst.start() == pynini.NO_STATE_ID:
        raise ValueError("Rewrite failed — check input format.")
    return pynini.shortestpath(output_fst).string(token_type="utf8")


def normalize_text( text , lang):
    
    normalized_with_tag = classify_sentence(text, lang)
    normalized = strip_tags_with_pynutil(normalized_with_tag)
    print("text before normalization: ",text)
    print("text normalized: ", normalized)
    

if __name__ == '__main__':

    # 1. Create an ArgumentParser object
    parser = argparse.ArgumentParser(description="Variables required to use the number normalizer")

    # 2. Add arguments
    parser.add_argument("--text", help="The text to be normalized")
    parser.add_argument("--lang", default='eng', help="The language for normalization. It could be 'fre' for French or 'eng' for English (Default is 'en')")

    # 3. Parse the arguments
    args = parser.parse_args()

    if not args.lang in ['eng','fre']:
        raise ValueError(' Make sure the language is either "eng" or "fre"')
    
    text = args.text or "we started with the number 100 and ended up 823."
    lang = args.lang or 'eng'

    start = time.time()
    normalize_text(text, lang)
    end = time.time() 
    print('runtime:', end - start, 'seconds')


