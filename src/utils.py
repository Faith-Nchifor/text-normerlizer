import os
import pynini
from pynini.lib import pynutil, utf8

# create a directory to store far files
far_dir = 'data'
os.makedirs(far_dir,exist_ok=True)

far_path = os.path.join(far_dir,'fst.far')

# This function creates an FST that transduces a single input string to a single output string.

def I_O_FST(input_str: str, output_str: str):#, table: pynini.SymbolTable -> pynini.Fst:
    """Creates an FST mapping input_str to output_str."""

    # Ensure inputs are strings
    input_str = str(input_str)
    output_str = str(output_str)
    #Create two FSAs one for the input the other for the output alphabet
    input_accep = pynini.accep(input_str, token_type="utf8")
    output_accep = pynini.accep(output_str,  token_type="utf8")

    # Create the cross-product (input:output transducer), pynini.cross creates an FST from two FSAs
    fst = pynini.cross(input_accep, output_accep)

    return fst.optimize()


# function normalizes text
def apply_fst(text, fst):
    '''finds the best FST for input'''
    try:
        return(pynini.shortestpath(pynini.accep(text,token_type='utf8') @ fst).string("utf8"))

    except Exception as e:
        return(f"Error: {e}, for input:'{text}'")
    

#  Some variables and functions used in generating the normalized text

digits = pynini.closure(pynini.union(*[pynini.accep(chr(i)) for i in range(ord('0'), ord('9')+1)]), 1)
letters = pynini.closure(pynini.union(*[pynini.accep(chr(i)) for i in range(ord('a'), ord('z')+1)] +
                                       [pynini.accep(chr(i)) for i in range(ord('A'), ord('Z')+1)]), 1)
apostrophe = pynini.union("'", "’")

word = letters + pynini.closure(apostrophe + letters)  # e.g. n'iminota, can't, etc.
number = digits
punctuation = pynini.union(*[pynini.accep('\\' +c) for c in ".,:;!?-(){}[]\""])

token = number | word | punctuation
def tokenize_pynini(text: str) -> list[str]:
    lattice = pynini.accep(text)
    tokens = []
    start = 0
    while start < len(text):
        # Try to find the longest token starting at `start`
        longest_token = None
        longest_len = 0
        for length in range(1, len(text) - start + 1):
            segment = text[start:start+length]
            if pynini.compose(segment, token).num_states() > 0:
                # Accept if segment matches token
                longest_token = segment
                longest_len = length
        if longest_token is None:
            # No token found, consume one char as unknown token
            longest_token = text[start]
            longest_len = 1
        tokens.append(longest_token)
        start += longest_len
    return tokens

# This part takes the individual tokens generated by the tokenizer and iteratively apply the FST to convert number to text.:
# it returns values that looks like this: { word { value: "we" } }  { cardinal { integer: "six" } }}. Where different classes have different tags, and only the chooses classes are affected by the normalizer.
# in our case only the values with tags "cardinal"
def classify_token( token: str) -> str:
    print(type(token))
    if token.isdigit():
        verbalized = apply_fst(token,fsts)
        # verbalized = pynini.shortestpath(
            # pynini.accep(token, token_type="utf8") @ rewrite_fst
        # ).string(token_type="utf8")
        return f'{{ cardinal {{ integer: "{verbalized}" }} }}'
    else:
        return f'{{ word {{ value: "{token}" }} }}'

def classify_sentence(text: str) -> str:
    tokens = tokenize_pynini(text)
    return " ".join(classify_token(token) for token in tokens)



QUOTE = pynini.accep('"')
NOT_QUOTE = pynini.difference(utf8.VALID_UTF8_CHAR, QUOTE)
VALUE = pynini.closure(NOT_QUOTE, 1)

# Match: { word { value: "<value>" } }
word_tag = pynutil.delete('{ word { value: "') + VALUE + pynutil.delete('" } }')

# Match: { cardinal { integer: "<value>" } }
cardinal_tag = pynutil.delete('{ cardinal { integer: "') + VALUE + pynutil.delete('" } }')

# Combine and rewrite globally
space_token = pynutil.delete('{ word { value: "')+ VALUE + pynutil.delete('" } }')
SPACE_BETWEEN_TAGS = pynutil.delete(" ")
tag_remover = pynini.union(word_tag, cardinal_tag, space_token,SPACE_BETWEEN_TAGS)
cleaner_fst = pynini.cdrewrite(tag_remover, "", "", pynini.closure(utf8.VALID_UTF8_CHAR)).optimize()


def strip_tags_with_pynutil(tagged_text: str) -> str:
    input_fst = pynini.accep(tagged_text, token_type="utf8")
    output_fst = input_fst @ cleaner_fst
    if output_fst.start() == pynini.NO_STATE_ID:
        raise ValueError("Rewrite failed — check input format.")
    return pynini.shortestpath(output_fst).string(token_type="utf8")


def normalize_text(fst, text = "we started with the number 100 and ended up 823."):

    normalized_with_tag = classify_sentence(text, fst)
    normalized = strip_tags_with_pynutil(normalized_with_tag)
    print("text before normalization: ",text)
    # print("text normalized - but still taged: ", normalized_with_tag)
    print("text normalized: ", normalized)

